[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
<a href=""> <img src="https://img.shields.io/github/stars/koo-ec/Awesome-LLM-Explainability?style=flat-square&logo=github" alt="GitHub stars"></a>
<a href=""> <img src="https://img.shields.io/github/forks/koo-ec/Awesome-LLM-Explainability?style=flat-square&logo=github" alt="GitHub forks"></a>
<a href=""> <img src="https://img.shields.io/github/issues/koo-ec/Awesome-LLM-Explainability?style=flat-square&logo=github" alt="GitHub issues"></a>
<a href=""> <img src="https://img.shields.io/github/last-commit/koo-ec/Awesome-LLM-Explainability?style=flat-square&logo=github" alt="GitHub Last commit"></a>
<a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT"></a>
<a href="https://standardjs.com"><img src="https://img.shields.io/badge/code_style-standard-brightgreen.svg" alt="Standard - \Python Style Guide"></a>  
</p>

# Awesome-LLM-Explainability
<p align="justify">A curated list of explainability-related papers, articles, and resources focused on Large Language Models (LLMs). This repository aims to provide researchers, practitioners, and enthusiasts with insights into the explainability implications, challenges, and advancements surrounding these powerful models.</p>

## Introduction
<p align="justify">We've curated a collection of the latest ğŸ“ˆ, most comprehensive ğŸ“š, and most valuable ğŸ’¡ resources on large language model explainability (LLM Explainability)). But we don't stop there; included are also relevant talks, tutorials, conferences, news, and articles. Our repository is constantly updated to ensure you have the most current information at your fingertips.</p>

## Table of Contents
- [Awesome LLM-Explainability](#ï¸awesome-llm-xai)
  - [Introduction](#intro)
  - [Table of Contents](#table-of-contents)
  - [Articles](#papers)
    - [ğŸ“‘Papers](#papers)
    - [ğŸ“–Articles, and Presentations](#articles)
    - [Other](#other)
  - [Datasets \& Benchmark](#datasets--benchmark)
    - [ğŸ“‘Papers](#papers-4)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-5)
    - [ğŸ“šResource](#resource)
    - [Other](#other-5)
  - [ğŸ‘©â€ğŸ« Scholars](#-scholars)
  - [ğŸ§‘â€ğŸ“Contributors](#author)

## Neural Network Analysis
| Date       | Institute | Publication | Paper Title |
|------------|-----------|-------------|-------------|
| YYYY-MM-DD | Institute | Journal     | Finding Neurons in a Haystack: Case Studies with Sparse Probing |
| YYYY-MM-DD | Institute | Journal     | Copy Suppression: Comprehensively Understanding an Attention Head |
| YYYY-MM-DD | Institute | Journal     | Towards Automated Circuit Discovery for Mechanistic Interpretability |
| YYYY-MM-DD | Institute | Journal     | Language models can explain neurons in language models |
| YYYY-MM-DD | Institute | Journal     | Toward a Mechanistic Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model |
| YYYY-MM-DD | Institute | Journal     | Successor Heads: Recurring, Interpretable Attention Heads In The Wild |
| YYYY-MM-DD | Institute | Journal     | Neurons in Large Language Models: Dead, N-gram, Positional |
| YYYY-MM-DD | Institute | Journal     | Interpretability in the Wild: GPT-2 small (arXiv) |
| YYYY-MM-DD | Institute | Journal     | Explaining black box text modules in natural language with language models |
| YYYY-MM-DD | Institute | Journal     | N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models |
| YYYY-MM-DD | Institute | Journal     | Interpreting GPT: the logit lens |

## Algorithmic Approaches
| Date       | Institute | Publication | Paper Title |
|------------|-----------|-------------|-------------|
| YYYY-MM-DD | Institute | Journal     | Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias |
| YYYY-MM-DD | Institute | Journal     | Discovering Latent Knowledge in Language Models Without Supervision |
| YYYY-MM-DD | Institute | Journal     | Towards Monosemanticity: Decomposing Language Models With Dictionary Learning |
| YYYY-MM-DD | Institute | Journal     | Spine: Sparse interpretable neural embeddings |
| YYYY-MM-DD | Institute | Journal     | Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors |
| YYYY-MM-DD | Institute | Journal     | Sparse Autoencoders Find Highly Interpretable Features in Language Models |
| YYYY-MM-DD | Institute | Journal     | Attribution Patching: Activation Patching At Industrial Scale |
| YYYY-MM-DD | Institute | Journal     | Causal Scrubbing: a method for rigorously testing interpretability hypotheses [Redwood Research] |

## Representation Analysis
| Date       | Institute | Publication | Paper Title |
|------------|-----------|-------------|-------------|
| YYYY-MM-DD | Institute | Journal     | Linear Representations of Sentiment in Large Language Models |
| YYYY-MM-DD | Institute | Journal     | Emergent Linear Representations in World Models of Self-Supervised Sequence Models |
| YYYY-MM-DD | Institute | Journal     | Measuring Feature Sparsity in Language Models |
| YYYY-MM-DD | Institute | Journal     | Polysemanticity and capacity in neural networks |
| YYYY-MM-DD | Institute | Journal     | Visualizing and measuring the geometry of BERT |
| YYYY-MM-DD | Institute | Journal     | The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets |

## Bias and Robustness Studies
| Date       | Institute | Publication | Paper Title |
|------------|-----------|-------------|-------------|
| YYYY-MM-DD | Institute | Journal     | Large Language Models Are Not Robust Multiple Choice Selectors |
| YYYY-MM-DD | Institute | Journal     | The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models |
| YYYY-MM-DD | Institute | Journal     | ChainPoll: A High Efficacy Method for LLM Hallucination Detection |
| YYYY-MM-DD | Institute | Journal     | Evaluating LLMs is a minefield |

## Interpretability Frameworks
| Date       | Institute | Publication | Paper Title |
|------------|-----------|-------------|-------------|
| YYYY-MM-DD | Institute | Journal     | Let's Verify Step by Step |
| YYYY-MM-DD | Institute | Journal     | Interpretability Illusions in the Generalization of Simplified Models |
| YYYY-MM-DD | Institute | Journal     | Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling |
| YYYY-MM-DD | Institute | Journal     | Can Large Language Models Explain Themselves? |
| YYYY-MM-DD | Institute | Journal     | A Mechanistic Interpretability Analysis of Grokking |
| YYYY-MM-DD | Institute | Journal     | 200 Concrete Open Problems in Mechanistic Interpretability |
| YYYY-MM-DD | Institute | Journal     | Interpretability at Scale: Identifying Causal Mechanisms in Alpaca |
| YYYY-MM-DD | Institute | Journal     | Representation Engineering: A Top-Down Approach to AI Transparency |
| YYYY-MM-DD | Institute | Journal     | Augmenting Interpretable Models with LLMs during Training |

## Application-Specific Studies
| Date       | Institute | Publication | Paper Title |
|------------|-----------|-------------|-------------|
| YYYY-MM-DD | Institute | Journal     | Emergent world representations: Exploring a sequence model trained on a synthetic task |
| YYYY-MM-DD | Institute | Journal     | How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model |
| YYYY-MM-DD | Institute | Journal     | Interpreting the Inner Mechanisms of Large Language Models in Mathematical Addition |
| YYYY-MM-DD | Institute | Journal     | An Overview of Early Vision in InceptionV1 |

## Theoretical Approaches
| Date       | Institute | Publication | Paper Title |
|------------|-----------|-------------|-------------|
| YYYY-MM-DD | Institute | Journal     | A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations |
| YYYY-MM-DD | Institute | Journal     | The Quantization Model of Neural Scaling |
| YYYY-MM-DD | Institute | Journal     | Toy Models of Superposition |
| YYYY-MM-DD | Institute | Journal     | Engineering monosemanticity in toy models |
| YYYY-MM-DD | Institute | Journal     | A New Approach to Computation Reimagines Artificial Intelligenceg |
 

<!--
**ğŸ§‘â€ğŸ’» Our Work**


> If a resource is relevant to multiple subcategories, we place it under each applicable section. For instance, the "Awesome-LLM-Safety" repository will be listed under each subcategory to which it pertainsğŸ¤©!.

**âœ”ï¸ Perfect for Majority**
- For beginners curious about llm-safety, our repository serves as a compass for grasping the big picture and diving into the details. Classic or influential papers retained in the README provide a beginner-friendly navigation through interesting directions in the field;
- For seasoned researchers, this repository is a tool to keep you informed and fill any gaps in your knowledge. Within each subtopic, we are diligently updating all the latest content and continuously backfilling with previous work. Our thorough compilation and careful selection are time-savers for you.

**ğŸ§­ How to Use this Guide**
- Quick Start: In the README, users can find a curated list of select information sorted by date, along with links to various consultations.
- In-Depth Exploration: If you have a special interest in a particular subtopic, delve into the "subtopic" folder for more. Each item, be it an article or piece of news, comes with a brief introduction, allowing researchers to swiftly zero in on relevant content.


**Letâ€™s start LLM Safety tutorial!**

---

## ğŸš€Table of Contents

- [ğŸ›¡ï¸Awesome LLM-SafetyğŸ›¡ï¸](#ï¸awesome-llm-safetyï¸)
  - [ğŸ¤—Introduction](#introduction)
  - [ğŸš€Table of Contents](#table-of-contents)
  - [ğŸ”Security](#security)
    - [ğŸ“‘Papers](#papers)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks)
    - [Other](#other)
  - [ğŸ”Privacy](#privacy)
    - [ğŸ“‘Papers](#papers-1)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-1)
    - [Other](#other-1)
  - [ğŸ“°Truthfulness \& Misinformation](#truthfulness--misinformation)
    - [ğŸ“‘Papers](#papers-2)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-2)
    - [Other](#other-2)
  - [ğŸ˜ˆJailBreak \& Attacks](#jailbreak--attacks)
    - [ğŸ“‘Papers](#papers-3)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-3)
    - [Other](#other-3)
  - [ğŸ›¡ï¸Defenses](#ï¸defenses)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-4)
    - [Other](#other-4)
  - [ğŸ’¯Datasets \& Benchmark](#datasets--benchmark)
    - [ğŸ“‘Papers](#papers-4)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-5)
    - [ğŸ“šResourceğŸ“š](#resource)
    - [Other](#other-5)
  - [ğŸ§‘â€ğŸ« Scholars ğŸ‘©â€ğŸ«](#-scholars-)
  - [ğŸ§‘â€ğŸ“Author](#author)



---
## ğŸ”Security 

### ğŸ“‘Papers
| Date  |      Institute       | Publication |                                                                                            Paper                                                                                            |
|:-----:|:--------------------:|:-----------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| 20.10 | Facebook AI Research |    arxiv    |                                                       [Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)                                                        |
| 22.03 |        OpenAI        |  NIPS2022   | [Training language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html) |
| 23.07 |     UC Berkeley      |  NIPS2023   |                                                     [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)                                                      |
| 23.12 |        OpenAI        |   Open AI   |                                 [Practices for Governing Agentic AI Systems](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf)                                  |

### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |          Type          |        Title         |                                          URL                                          |
|:-----:|:----------------------:|:--------------------:|:-------------------------------------------------------------------------------------:|
| 22.02 | Toxicity Detection API |   Perspective API    | [link](https://www.perspectiveapi.com/)<br/>[paper](https://arxiv.org/abs/2202.11176) |
| 23.07 |       Repository       | Awesome LLM Security |               [link](https://github.com/corca-ai/awesome-llm-security)                |
| 23.10 |       Tutorials        |  Awesome-LLM-Safety  |                 [link](https://github.com/ydyjya/Awesome-LLM-Safety)                  |


### Other

ğŸ‘‰[Latest&Comprehensive Security Paper](.//subtopic/Security.md)

---
## ğŸ”Privacy 


### ğŸ“‘Papers
| Date  |    Institute    | Publication |                                                            Paper                                                             |
|:-----:|:---------------:|:-----------:|:----------------------------------------------------------------------------------------------------------------------------:|
| 19.12 |    Microsoft    |   CCS2020   |  [Analyzing Information Leakage of Updates to Natural Language Models](https://dl.acm.org/doi/abs/10.1145/3372297.3417880)   |
| 21.07 | Google Research |   ACL2022   |           [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)            |
| 21.10 |    Stanford     |  ICLR2022   |      [Large language models can be strong differentially private learners](https://openreview.net/forum?id=bVuP3ltATMz)      |
| 22.02 | Google Research |  ICLR2023   |             [Quantifying Memorization Across Neural Language Models](https://openreview.net/forum?id=TatRHT_1cK)             |
| 22.02 | UNC Chapel Hill |  ICML2022   | [Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://proceedings.mlr.press/v162/kandpal22a.html) |

### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |


### Other

ğŸ‘‰[Latest&Comprehensive Privacy Paper](.//subtopic/Privacy.md)

---
## ğŸ“°Truthfulness & Misinformation 


### ğŸ“‘Papers
| Date  |           Institute            | Publication |                                                                    Paper                                                                     |
|:-----:|:------------------------------:|:-----------:|:--------------------------------------------------------------------------------------------------------------------------------------------:|
| 21.09 |      University of Oxford      |   ACL2022   |                         [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                          |
| 23.11 | Harbin Institute of Technology |    arxiv    | [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232) |
| 23.11 |    Arizona State University    |    arxiv    |                      [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                      |

### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |    Type    |          Title           |                                URL                                |
|:-----:|:----------:|:------------------------:|:-----------------------------------------------------------------:|
| 23.07 | Repository | llm-hallucination-survey | [link](https://github.com/HillZhang1999/llm-hallucination-survey) |
| 23.10 | Repository |  LLM-Factuality-Survey   |   [link](https://github.com/wangcunxiang/LLM-Factuality-Survey)   |
| 23.10 | Tutorials  |    Awesome-LLM-Safety    |       [link](https://github.com/ydyjya/Awesome-LLM-Safety)        |

### Other

ğŸ‘‰[Latest&Comprehensive Truthfulness&Misinformation Paper](./subtopic/Truthfulness&Misinformation.md)

---
## ğŸ˜ˆJailBreak & Attacks 

### ğŸ“‘Papers
| Date  | Institute |         Publication          |                                                                   Paper                                                                   |
|:-----:|:---------:|:----------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------:|
| 20.12 |  Google   |     USENIX Security 2021     | [Extracting Training Data from Large Language Models](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting) |
| 22.11 | AE Studio | NIPS2022(ML Safety Workshop) |                     [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)                     |
| 23.06 |  Google   |            arxiv             |                          [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)                           |
| 23.07 |    CMU    |            arxiv             |               [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)               |

### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |        Type        |                                       Title                                       |                                  URL                                  |
|:-----:|:------------------:|:---------------------------------------------------------------------------------:|:---------------------------------------------------------------------:|
| 23.01 |     Community      |                              Reddit/ChatGPTJailbrek                               |           [link](https://www.reddit.com/r/ChatGPTJailbreak)           |
| 23.02 | Resource&Tutorials |                                  Jailbreak Chat                                   |                [link](https://www.jailbreakchat.com/)                 |
| 23.10 |     Tutorials      |                                Awesome-LLM-Safety                                 |         [link](https://github.com/ydyjya/Awesome-LLM-Safety)          |
| 23.10 |      Article       |                 Adversarial Attacks on LLMs(Author: Lilian Weng)                  | [link](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/) |
| 23.11 |       Video        | [1hr Talk] Intro to Large Language Models<br/>From 45:45(Author: Andrej Karpathy) |          [link](https://www.youtube.com/watch?v=zjkBMFhNj_g)          |

### Other

ğŸ‘‰[Latest&Comprehensive JailBreak & Attacks Paper](./subtopic/Jailbreaks&Attack.md)

---
## ğŸ›¡ï¸Defenses 

### ğŸ“‘Papers
| Date  |    Institute    | Publication |                                                             Paper                                                             |
|:-----:|:---------------:|:-----------:|:-----------------------------------------------------------------------------------------------------------------------------:|
| 21.07 | Google Research |   ACL2022   |            [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)            |
| 22.04 |    Anthropic    |    arxiv    | [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862) |



### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |    Type    |         Title         |                              URL                              |
|:-----:|:----------:|:---------------------:|:-------------------------------------------------------------:|
| 23.10 | Tutorials  |  Awesome-LLM-Safety   |     [link](https://github.com/ydyjya/Awesome-LLM-Safety)      |

### Other

ğŸ‘‰[Latest&Comprehensive Defenses Paper](./subtopic/Defense.md)


--- 
## ğŸ’¯Datasets & Benchmark

### ğŸ“‘Papers
| Date  |        Institute         |     Publication     |                                                                  Paper                                                                   |
|:-----:|:------------------------:|:-------------------:|:----------------------------------------------------------------------------------------------------------------------------------------:|
| 20.09 | University of Washington | EMNLP2020(findings) |             [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)             |
| 21.09 |   University of Oxford   |       ACL2022       |                       [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                        |
| 22.03 |           MIT            |       ACL2022       | [ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509) |


### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

### ğŸ“šResourceğŸ“š
- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)

### Other
ğŸ‘‰[Latest&Comprehensive datasets & Benchmark Paper](./subtopic/Datasets&Benchmark.md)

---
## ğŸ§‘â€ğŸ« Scholars ğŸ‘©â€ğŸ« 
**In this section, we list some of the scholars we consider to be experts in the field of LLM Safety!**

|     Scholars     |                                                                     HomePage&Google Scholars                                                                     |                                                                                                      Keywords or Interested                                                                                                       |
|:----------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| Nicholas Carlini | [Homepage](https://nicholas.carlini.com/) \| [Google Scholar](https://scholar.google.com/citations?hl=zh-CN&user=q4qDvAoAAAAJ&view_op=list_works&sortby=pubdate) |                                                        **the intersection of machine learning and computer security**&**neural networks from an adversarial perspective**                                                         |
| Daphne Ippolito  |                       [Google Scholar](https://scholar.google.com/citations?hl=zh-CN&user=COEsqLYAAAAJ&view_op=list_works&sortby=pubdate)                        |                                                                                                  **Natural Language Processing**                                                                                                  |
|  Chiyuan Zhang   |     [Homepage](https://pluskid.org/) \| [Google Scholar](https://scholar.google.com/citations?hl=zh-CN&user=l_G2vr0AAAAJ&view_op=list_works&sortby=pubdate)      |                                **Especially interested in understanding the generalization and memorization in machine and human learning, as well as implications in related areas like privacy**                                |
|  Katherine Lee   |                       [Google Scholar](https://scholar.google.com/citations?hl=zh-CN&user=bjdB4K8AAAAJ&view_op=list_works&sortby=pubdate)                        |                                                           **natural language processing**&**translation**&**machine learning**&**computational neuroscienceattention**                                                            |
|  Florian TramÃ¨r  |                   [Homepage](https://floriantramer.com/) \| [Google Scholar](https://scholar.google.com/citations?hl=zh-CN&user=ijH0-a8AAAAJ)                    | **Computer Security**&**Machine Learning**&**Cryptography**&**the worst-case behavior of Deep Learning systems from an adversarial perspective, to understand and mitigate long-term threats to the safety and privacy of users** |
|   Jindong Wang   |      [Homepage](https://scholar.google.com/citations?hl=zh-CN&user=hBZ_tKsAAAAJ&view_op=list_works&sortby=pubdate) \| [Google Scholar](https://jd92.wang/)       |                                                                             **Large Language Models (LLMs) evaluation and robustness enhancement** ã€€                                                                              |
|   Chaowei Xiao   |                       [Homepage](https://xiaocw11.github.io/) \| [Google Scholar](https://scholar.google.com/citations?user=Juoqtj8AAAAJ)                        |                                     **interested in exploring the trustworthy problem in (MultiModal) Large Language Models and studying the role of LLMs in different application domains.**                                     |
|     Andy Zou     |                  [Homepage](https://andyzoujm.github.io/) \| [Google Scholar](https://scholar.google.com/citations?hl=zh-CN&user=zir09KwAAAAJ)                   |                                                                                                    **ML Safety**&**AI Safety**                                                                                                    |

---
## ğŸ§‘â€ğŸ“Author


**ğŸ¤—If you have any questions, please contact our authors!ğŸ¤—**

âœ‰ï¸: [ydyjya](https://github.com/ydyjya) â¡ï¸ zhouzhenhong@bupt.edu.cn

ğŸ’¬: **LLM Safety Discussion**

<div align="center">

[Wechat Group](./resource/wechat.png) | [My Wechat](./resource/wechat.png)

</div>


---

[![Star History Chart](https://api.star-history.com/svg?repos=ydyjya/Awesome-LLM-Explainability&type=Date)](https://star-history.com/#koo-ec/Awesome-LLM-Explainability&Date)

**[â¬† Back to ToC](#table-of-contents)**

-->

## Future Research
One future direction is fairness-explainability for LLM.
